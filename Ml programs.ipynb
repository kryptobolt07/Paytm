{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvvUWvU9o9O22HLhXam2sL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kryptobolt07/Paytm/blob/main/Ml%20programs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_36PV0q19V0"
      },
      "outputs": [],
      "source": [
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pprint, time\n",
        "\n",
        "\n",
        "nltk.download('treebank')\n",
        "o\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n",
        "\n",
        "\n",
        "print(nltk_data[:2])\n",
        "\n",
        "for sent in nltk_data[:2]:\n",
        "    for tuple in sent:\n",
        "        print(tuple)\n",
        "\n",
        "\n",
        "train_set, test_set =train_test_split(nltk_data,train_size=0.80,test_size=0.20,random_state = 101)\n",
        "\n",
        "\n",
        "train_tagged_words = [tup for sent in train_set for tup in sent ]\n",
        "test_tagged_words = [tup for sent in test_set for tup in sent ]\n",
        "print(len(train_tagged_words))\n",
        "print(len(test_tagged_words))\n",
        "\n",
        "# check some of the tagged words.\n",
        "print(train_tagged_words[:5])\n",
        "\n",
        "# use set datatype to check how many unique tags are present in training data\n",
        "tags = {tag for word,tag in train_tagged_words}\n",
        "print(len(tags))\n",
        "print(tags)\n",
        "\n",
        "# check total words in vocabulary\n",
        "vocab = {word for word,tag in train_tagged_words}\n",
        "\n",
        "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
        "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
        "    count_tag = len(tag_list) #total number of times the passed tag occurred in train_bag\n",
        "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
        "\n",
        "    count_w_given_tag = len(w_given_tag_list)\n",
        "\n",
        "    return (count_w_given_tag, count_tag)\n",
        "\n",
        "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
        "    tags_list = [pair[1] for pair in train_bag]\n",
        "    count_t1 = len([t for t in tags_list if t==t1])\n",
        "    count_t2_t1 = 0\n",
        "    for index in range(len(tags_list)-1):\n",
        "        if tags_list[index] == t1 and tags_list[index+1] == t2:\n",
        "            count_t2_t1 += 1\n",
        "    return (count_t2_t1, count_t1)\n",
        "\n",
        "\n",
        "tags_list = list(tags)\n",
        "tags_matrix = np.zeros((len(tags_list), len(tags_list)), dtype='float32')\n",
        "for i, t1 in enumerate(tags_list):\n",
        "    for j, t2 in enumerate(tags_list):\n",
        "        count_t2_t1, count_t1 = t2_given_t1(t2, t1)\n",
        "        tags_matrix[i, j] = count_t2_t1 / count_t1 if count_t1 != 0 else 0\n",
        "\n",
        "print(tags_matrix)\n",
        "\n",
        "\n",
        "tags_df = pd.DataFrame(tags_matrix, columns = list(tags), index=list(tags))\n",
        "\n",
        "print(tags_df)\n",
        "\n",
        "def Viterbi(words, train_bag = train_tagged_words):\n",
        "    state = []\n",
        "    T = list(set([pair[1] for pair in train_bag]))\n",
        "\n",
        "    for key, word in enumerate(words):\n",
        "        p = []\n",
        "        for tag in T:\n",
        "            if key == 0:\n",
        "                transition_p = tags_df.loc['.', tag]\n",
        "            else:\n",
        "                transition_p = tags_df.loc[state[-1], tag]\n",
        "\n",
        "            count_w_given_tag, count_tag = word_given_tag(word, tag)\n",
        "            # Added check for division by zero\n",
        "            emission_p = count_w_given_tag / count_tag if count_tag != 0 else 0\n",
        "\n",
        "            state_probability = emission_p * transition_p\n",
        "            p.append(state_probability)\n",
        "\n",
        "        pmax = max(p)\n",
        "        state_max = T[p.index(pmax)]\n",
        "        state.append(state_max)\n",
        "\n",
        "    return list(zip(words, state))\n",
        "\n",
        "\n",
        "random.seed(1234)\n",
        "\n",
        "\n",
        "rnom = [random.randint(0,len(test_set) - 1) for x in range(10)] #\n",
        "\n",
        "\n",
        "test_run = [test_set[i] for i in rnom]\n",
        "\n",
        "test_run_base = [tup for sent in test_run for tup in sent]\n",
        "\n",
        "test_tagged_words = [tup[0] for sent in test_run for tup in sent]\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "tagged_seq = Viterbi(test_tagged_words)\n",
        "end = time.time()\n",
        "difference = end-start\n",
        "\n",
        "print(\"Time taken in seconds: \", difference)\n",
        "\n",
        "\n",
        "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j]\n",
        "\n",
        "accuracy = len(check)/len(tagged_seq)\n",
        "print('Viterbi Algorithm Accuracy: ',accuracy*100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def jaccard_similarity(a, b):\n",
        "    a_set, b_set = set(a.lower().split()), set(b.lower().split())\n",
        "    return len(a_set & b_set) / len(a_set | b_set)\n",
        "\n",
        "def tfidf_cosine_similarity(a, b):\n",
        "    vec = TfidfVectorizer()\n",
        "    tfidf = vec.fit_transform([a, b])\n",
        "    return cosine_similarity(tfidf[0:1], tfidf[1:2])[0][0]\n",
        "\n",
        "def word2vec_similarity(a, b):\n",
        "    a_tok, b_tok = word_tokenize(a.lower()), word_tokenize(b.lower())\n",
        "    model = Word2Vec([a_tok, b_tok], vector_size=100, window=3, min_count=1, workers=1)\n",
        "    wa = sum(model.wv[w] for w in a_tok) / len(a_tok)\n",
        "    wb = sum(model.wv[w] for w in b_tok) / len(b_tok)\n",
        "    return cosine_similarity([wa], [wb])[0][0]\n",
        "\n",
        "a = \"My laptop screams louder than me during exams.\"\n",
        "b = \"My laptop cries every time I open Chrome.\"\n",
        "\n",
        "print(\"\\nJaccard Similarity:\", jaccard_similarity(a, b))\n",
        "print(\"TF-IDF Cosine Similarity:\", tfidf_cosine_similarity(a, b))\n",
        "print(\"Word2Vec Similarity:\", word2vec_similarity(a, b))"
      ],
      "metadata": {
        "id": "9tFy7hFh2Evs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.util import ngrams\n",
        "\n",
        "text = \"\"\"\n",
        "The phone is fast.\n",
        "The phone is smooth.\n",
        "The phone is powerful.\n",
        "The phone is reliable.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = [w.lower() for w in re.findall(r\"\\b\\w+\\b\", text)]\n",
        "\n",
        "# Bigram Model\n",
        "N = 2\n",
        "train_data, vocab = padded_everygram_pipeline(N, [tokens])\n",
        "\n",
        "lm_bigram = MLE(N)\n",
        "lm_bigram.fit(train_data, vocab)\n",
        "\n",
        "print(\"\\nN=2 (Bigram Model)\")\n",
        "bigram_counts = {}\n",
        "for token in set(tokens):\n",
        "    bigram_counts[token] = dict(lm_bigram.counts[(token,)])\n",
        "print(\"All bigram counts:\", bigram_counts)\n",
        "\n",
        "# Prediction - 3 words\n",
        "context = 'the'\n",
        "predicted_words = [context]\n",
        "for i in range(3):\n",
        "    if lm_bigram.counts[(context,)]:\n",
        "        next_word = max(lm_bigram.counts[(context,)].items(), key=lambda x: x[1])[0]\n",
        "        predicted_words.append(next_word)\n",
        "        context = next_word\n",
        "print(f\"Bigram prediction: {' '.join(predicted_words)}\")\n",
        "\n",
        "# Trigram Model\n",
        "N = 3\n",
        "train_data, vocab = padded_everygram_pipeline(N, [tokens])\n",
        "\n",
        "lm_trigram = MLE(N)\n",
        "lm_trigram.fit(train_data, vocab)\n",
        "\n",
        "print(\"\\nN=3 (Trigram Model)\")\n",
        "trigram_counts = {}\n",
        "bigrams_in_text = list(ngrams(tokens, 2))\n",
        "for bg in set(bigrams_in_text):\n",
        "    trigram_counts[bg] = dict(lm_trigram.counts[bg])\n",
        "print(\"All trigram counts:\", trigram_counts)\n",
        "\n",
        "# Prediction - 3 words\n",
        "context = ['the']\n",
        "predicted_words = context.copy()\n",
        "for i in range(3):\n",
        "    if lm_trigram.counts[tuple(context)]:\n",
        "        next_word = max(lm_trigram.counts[tuple(context)].items(), key=lambda x: x[1])[0]\n",
        "        predicted_words.append(next_word)\n",
        "        context = [context[0], next_word]\n",
        "print(f\"Trigram prediction: {' '.join(predicted_words)}\")"
      ],
      "metadata": {
        "id": "k4iF-P4_2HKc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}