{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/ZHkebyMK9ogQZqGMY7f+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kryptobolt07/Paytm/blob/main/ML_programs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 2: Implement Named Entity Recognition for any given text (NER)\n",
        "import nltk, spacy\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk import Tree # Utility for traversing NLTK NER output\n",
        "\n",
        "# Download necessary NLTK components for NER\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"maxent_ne_chunker\")\n",
        "nltk.download(\"words\")\n",
        "# Load spaCy model for comparison\n",
        "spacy.cli.download(\"en_core_web_sm\"); \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample Text for NER\n",
        "text = \"Dr. Elara Vance will present her research on Generative AI at the Google headquarters in London next Tuesday.\"\n",
        "\n",
        "# 1. NER using spaCy\n",
        "doc = nlp(text)\n",
        "# Extract entities (text and label)\n",
        "spacy_entities = [(e.text, e.label_) for e in doc.ents]\n",
        "print(\"spaCy Entities:\", spacy_entities)\n",
        "\n",
        "# 2. NER using NLTK (requires tokenization and POS tagging first)\n",
        "toks = word_tokenize(text)\n",
        "pos = pos_tag(toks) \n",
        "tree = ne_chunk(pos) # NE Chunking\n",
        "\n",
        "nltk_entities = []\n",
        "# Traverse the NLTK tree to extract named entity chunks\n",
        "for node in tree:\n",
        "    if isinstance(node, Tree):\n",
        "        # Join words in the chunk to form the entity name\n",
        "        name = \" \".join(w for w,t in node.leaves())\n",
        "        # Get the label (entity type)\n",
        "        label = node.label()\n",
        "        nltk_entities.append((name, label))\n",
        "print(\"NLTK Entities:\", nltk_entities)"
      ],
      "metadata": {
        "id": "ajm_gk71ToUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iPamMwqTcZO"
      },
      "outputs": [],
      "source": [
        "# Experiment 3: Perform morphological analysis (Stemming and Lemmatization) and Preprocessing\n",
        "import nltk, spacy\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download(\"punkt_tab\"); nltk.download(\"stopwords\"); nltk.download(\"wordnet\"); nltk.download(\"omw-1.4\")\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample Text\n",
        "text = \"Modern deep learning models are achieving state-of-the-art results on complex tasks faster than ever before.\"\n",
        "\n",
        "# 1. Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# 2. Stopwords Removal\n",
        "stopset = set(stopwords.words(\"english\"))\n",
        "tokens_no_stop = [t for t in tokens if t.isalnum() and t.lower() not in stopset]\n",
        "\n",
        "# 3. Stemming (Porter Stemmer)\n",
        "pst = PorterStemmer()\n",
        "stems = [pst.stem(t) for t in tokens]\n",
        "\n",
        "# 4. Lemmatization (NLTK WordNet Lemmatizer - simple form)\n",
        "wnl = WordNetLemmatizer()\n",
        "lemmas_nltk = [wnl.lemmatize(t) for t in tokens]\n",
        "\n",
        "# 5. Lemmatization (spaCy - typically more accurate due to POS context)\n",
        "doc = nlp(text)\n",
        "lemmas_spacy = [t.lemma_ for t in doc]\n",
        "\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Stopwords Removed:\", tokens_no_stop)\n",
        "print(\"Stems (Porter):\", stems)\n",
        "print(\"Lemmas (spaCy):\", lemmas_spacy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Experiment 4: Implement Chunking (Noun Phrase Chunking) for the given input text\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, RegexpParser\n",
        "\n",
        "# Download necessary NLTK components\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample Text for Chunking\n",
        "text = \"\"\"The newly developed quantum computer is a powerful machine that can process massive amounts of complex data instantaneously.\"\"\"\n",
        "\n",
        "# 1. Tokenization and POS Tagging\n",
        "words = word_tokenize(text)\n",
        "postags = pos_tag(words)\n",
        "print(\"POS Tags:\")\n",
        "print(postags)\n",
        "\n",
        "# 2. Define a Grammar for Chunking (Noun Phrase - NP)\n",
        "grammar = r\"\"\"\n",
        "  # NP: Optional Determiner (<DT>?), zero or more Adjectives (<JJ>*), one or more Nouns (<NN.*>+)\n",
        "  NP: {<DT>?<JJ>*<NN.*>+}\n",
        "\"\"\"\n",
        "\n",
        "# Create a RegexpParser object to implement the grammar\n",
        "chunk_parser = RegexpParser(grammar)\n",
        "\n",
        "# Apply the parser to the POS-tagged sentence to extract chunks\n",
        "tree = chunk_parser.parse(postags)\n",
        "\n",
        "print(\"\\nChunked Tree (Noun Phrases):\")\n",
        "print(tree)\n",
        "# The output tree explicitly shows the extracted NP chunks."
      ],
      "metadata": {
        "id": "uZlH48yqTux0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 5: Build a POS tagger using Hidden Markov Model (HMM) and Viterbi Algorithm\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Download the Penn Treebank corpus (used for training data)\n",
        "nltk.download('treebank')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "# Load the data\n",
        "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_set, test_set =train_test_split(nltk_data,train_size=0.80,test_size=0.20,random_state = 42)\n",
        "\n",
        "# Flatten the training set to a list of (word, tag) tuples\n",
        "train_tagged_words = [tup for sent in train_set for tup in sent ]\n",
        "\n",
        "# Get the set of unique tags\n",
        "tags = {tag for word,tag in train_tagged_words}\n",
        "tags_list = list(tags)\n",
        "print(f\"Unique Tags: {len(tags)}\\n{tags}\")\n",
        "\n",
        "# Function to calculate Emission Probability P(word | tag)\n",
        "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
        "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
        "    count_tag = len(tag_list) \n",
        "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
        "    count_w_given_tag = len(w_given_tag_list)\n",
        "    return (count_w_given_tag, count_tag)\n",
        "\n",
        "# Function to calculate Transition Probability P(t2 | t1)\n",
        "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
        "    tags_list = [pair[1] for pair in train_bag]\n",
        "    count_t1 = tags_list.count(t1)\n",
        "    count_t2_t1 = 0\n",
        "    for index in range(len(tags_list)-1):\n",
        "        if tags_list[index] == t1 and tags_list[index+1] == t2:\n",
        "            count_t2_t1 += 1\n",
        "    return (count_t2_t1, count_t1)\n",
        "\n",
        "# Build the Transition Probability Matrix (P(t2 | t1)) and DataFrame\n",
        "tags_matrix = np.zeros((len(tags_list), len(tags_list)), dtype='float32')\n",
        "for i, t1 in enumerate(tags_list):\n",
        "    for j, t2 in enumerate(tags_list):\n",
        "        count_t2_t1, count_t1 = t2_given_t1(t2, t1)\n",
        "        tags_matrix[i, j] = count_t2_t1 / count_t1 if count_t1 != 0 else 0\n",
        "tags_df = pd.DataFrame(tags_matrix, columns = tags_list, index=tags_list)\n",
        "\n",
        "print(\"\\nTransition Probability Matrix (Snippet):\\n\", tags_df.head())\n",
        "\n",
        "# Viterbi Algorithm Implementation\n",
        "def Viterbi(words, train_bag = train_tagged_words):\n",
        "    state = []\n",
        "    T = list(tags)\n",
        "    for k, word in enumerate(words):\n",
        "        p = {} \n",
        "        for tag in T:\n",
        "            if k == 0:\n",
        "                transition_p = tags_df.loc['.', tag] \n",
        "            else:\n",
        "                transition_p = tags_df.loc[state[-1], tag] \n",
        "            count_w_given_tag, count_tag = word_given_tag(word, tag)\n",
        "            emission_p = count_w_given_tag / count_tag if count_tag != 0 else 0\n",
        "            state_probability = emission_p * transition_p\n",
        "            p[tag] = state_probability\n",
        "        if p and max(p.values()) > 0:\n",
        "            state_max = max(p, key=p.get)\n",
        "        else:\n",
        "            state_max = 'NOUN' # Fallback\n",
        "        state.append(state_max)\n",
        "    return list(zip(words, state))\n",
        "\n",
        "# Test the model (using a small, random subset of the test data)\n",
        "random.seed(42) \n",
        "test_indices = [random.randint(0, len(test_set) - 1) for _ in range(5)]\n",
        "test_run = [test_set[i] for i in test_indices]\n",
        "test_run_base = [tup for sent in test_run for tup in sent] \n",
        "test_words = [tup[0] for tup in test_run_base] \n",
        "\n",
        "start = time.time()\n",
        "tagged_seq = Viterbi(test_words)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"\\nTime taken in seconds: {end-start:.2f}\")\n",
        "\n",
        "# Calculate Accuracy\n",
        "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j]\n",
        "accuracy = len(check)/len(tagged_seq)\n",
        "print(f'Viterbi Algorithm Accuracy: {accuracy*100:.2f}%')\n",
        "\n",
        "# Custom Sample Text Example\n",
        "custom_text = \"The research team achieved a breakthrough.\"\n",
        "custom_words = nltk.word_tokenize(custom_text)\n",
        "custom_tagged_seq = Viterbi(custom_words)\n",
        "print(\"\\nCustom Sentence Tagging:\")\n",
        "print(list(custom_tagged_seq))"
      ],
      "metadata": {
        "id": "ZLJccE0gT3TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 6: Similarity Detection in NLP (using TF-IDF and Cosine Similarity)\n",
        "\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Sample Texts to compare\n",
        "doc1 = \"The quick brown fox jumps over the lazy dog.\"\n",
        "doc2 = \"A speedy orange fox leaps above a tired canine.\"\n",
        "doc3 = \"The sky is blue and the grass is green.\"\n",
        "\n",
        "documents = [doc1, doc2, doc3]\n",
        "\n",
        "# 1. Initialize TF-IDF Vectorizer\n",
        "# It converts text into a matrix of token counts, weighted by their frequency in the document (TF) \n",
        "# and inversely proportional to their frequency across all documents (IDF).\n",
        "vectorizer = TfidfVectorizer(lowercase=True, stop_words='english')\n",
        "\n",
        "# 2. Compute TF-IDF Matrix\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# 3. Calculate Cosine Similarity\n",
        "# Cosine similarity measures the cosine of the angle between two non-zero vectors.\n",
        "cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "print(\"Document 1:\", doc1)\n",
        "print(\"Document 2:\", doc2)\n",
        "print(\"Document 3:\", doc3)\n",
        "print(\"\\nFeature Names (Vocabulary):\", vectorizer.get_feature_names_out())\n",
        "print(\"\\nCosine Similarity Matrix:\")\n",
        "# The matrix shows similarity: [0, 1] is Doc1 vs Doc2, [0, 2] is Doc1 vs Doc3, [1, 2] is Doc2 vs Doc3\n",
        "print(cosine_sim_matrix)\n",
        "\n",
        "# Print specific similarities\n",
        "print(f\"\\nSimilarity (Doc1 vs Doc2): {cosine_sim_matrix[0, 1]:.4f}\")\n",
        "print(f\"Similarity (Doc1 vs Doc3): {cosine_sim_matrix[0, 2]:.4f}\")\n",
        "print(f\"Similarity (Doc2 vs Doc3): {cosine_sim_matrix[1, 2]:.4f}\")"
      ],
      "metadata": {
        "id": "exp6_similarity"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 7: Implement N-Gram model (Bigram and Trigram) using Maximum Likelihood Estimation (MLE)\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.lm import MLE \n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "# Sample Training Text (Small Corpus)\n",
        "text = \"\"\"\n",
        "Artificial intelligence will change the world.\n",
        "Artificial intelligence is a branch of computer science.\n",
        "Computer science and technology drives innovation.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenization and Lowercasing\n",
        "tokens = [w.lower() for w in re.findall(r\"\\b\\w+\\b\", text)]\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# --- Bigram Model (N=2) ---\n",
        "N_bigram = 2\n",
        "# Prepare data: add start/end padding (<s>, </s>) and generate ngrams\n",
        "train_data_2, vocab_2 = padded_everygram_pipeline(N_bigram, [tokens])\n",
        "\n",
        "# Initialize and fit the MLE model\n",
        "lm_bigram = MLE(N_bigram)\n",
        "lm_bigram.fit(train_data_2, vocab_2)\n",
        "\n",
        "print(f\"\\nN={N_bigram} (Bigram Model - MLE)\")\n",
        "\n",
        "# Prediction: Generate the next 3 words starting from 'artificial'\n",
        "context_word = 'artificial'\n",
        "predicted_words_2 = [context_word]\n",
        "for i in range(3):\n",
        "    next_word_candidates_counts = lm_bigram.counts.get((context_word,))\n",
        "    if next_word_candidates_counts:\n",
        "        # MLE selects the word with the highest count\n",
        "        max_count_item = max(next_word_candidates_counts.items(), key=lambda x: x[1])\n",
        "        next_word = max_count_item[0]\n",
        "        predicted_words_2.append(next_word)\n",
        "        context_word = next_word # Update context\n",
        "    else:\n",
        "        break\n",
        "print(f\"Bigram prediction: {' '.join(predicted_words_2)}\")\n",
        "\n",
        "\n",
        "# --- Trigram Model (N=3) ---\n",
        "N_trigram = 3\n",
        "train_data_3, vocab_3 = padded_everygram_pipeline(N_trigram, [tokens])\n",
        "\n",
        "lm_trigram = MLE(N_trigram)\n",
        "lm_trigram.fit(train_data_3, vocab_3)\n",
        "\n",
        "print(f\"\\nN={N_trigram} (Trigram Model - MLE)\")\n",
        "\n",
        "# Prediction: Generate the next 3 words starting from 'artificial intelligence'\n",
        "context = ['artificial', 'intelligence'] \n",
        "predicted_words_3 = context.copy()\n",
        "for i in range(3):\n",
        "    context_tuple = tuple(context)\n",
        "    next_word_candidates_counts = lm_trigram.counts.get(context_tuple)\n",
        "\n",
        "    if next_word_candidates_counts:\n",
        "        # MLE selects the word with the highest count\n",
        "        max_count_item = max(next_word_candidates_counts.items(), key=lambda x: x[1])\n",
        "        next_word = max_count_item[0]\n",
        "        predicted_words_3.append(next_word)\n",
        "        # Update context: shift window\n",
        "        context = context[1:] + [next_word]\n",
        "    else:\n",
        "        break\n",
        "print(f\"Trigram prediction: {' '.join(predicted_words_3)}\")"
      ],
      "metadata": {
        "id": "hzt3l0F4T-Gd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}        "# Part 1: Hidden Markov Model (HMM) for Part-of-Speech Tagging using Viterbi Algorithm\n",
        "# Note: This section uses the NLTK Treebank corpus, so the \"sample text\" for the model itself is fixed, \n",
        "# but the test sample sentences are randomized.\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('treebank')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "# Get the Penn Treebank corpus tagged with Universal Tagset\n",
        "corpus_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n",
        "\n",
        "print(\"\\n--- Sample Tagged Sentences from Treebank ---\")\n",
        "print(corpus_data[:2])\n",
        "\n",
        "# Display the word/tag tuples individually for better inspection\n",
        "print(\"\\n--- Individual Word/Tag Pairs for First Two Sentences ---\")\n",
        "for sentence in corpus_data[:2]:\n",
        "    for word_tag_pair in sentence:\n",
        "        print(word_tag_pair)\n",
        "\n",
        "# Split the corpus into training and testing sets\n",
        "train_set, test_set = train_test_split(corpus_data, train_size=0.80, test_size=0.20, random_state=101)\n",
        "\n",
        "# Flatten the list of sentences into a single list of (word, tag) tuples for both sets\n",
        "train_tagged_words = [pair for sentence in train_set for pair in sentence]\n",
        "test_tagged_words_flat = [pair for sentence in test_set for pair in sentence] # Renamed to avoid confusion with words-only list later\n",
        "\n",
        "print(f\"\\nTotal tagged words in training data: {len(train_tagged_words)}\")\n",
        "print(f\"Total tagged words in testing data: {len(test_tagged_words_flat)}\")\n",
        "\n",
        "# Check the first few tagged words in the training data\n",
        "print(f\"\\nFirst 5 tagged words in training data: {train_tagged_words[:5]}\")\n",
        "\n",
        "# Determine the unique set of POS tags in the training data\n",
        "pos_tags = {tag for word, tag in train_tagged_words}\n",
        "print(f\"\\nNumber of unique tags: {len(pos_tags)}\")\n",
        "print(f\"Unique POS tags: {pos_tags}\")\n",
        "\n",
        "# Determine the total unique words (vocabulary) in the training data\n",
        "vocabulary = {word for word, tag in train_tagged_words}\n",
        "\n",
        "print(f\"Total unique words in vocabulary: {len(vocabulary)}\")\n",
        "\n",
        "\n",
        "# Function to calculate the probability of a word given a tag (Emission Probability P(word | tag))\n",
        "def word_given_tag_prob(word, tag, train_data=train_tagged_words):\n",
        "    # Filter pairs matching the given tag\n",
        "    tag_occurrences = [pair for pair in train_data if pair[1] == tag]\n",
        "    count_tag = len(tag_occurrences) # Count of the specific tag\n",
        "    \n",
        "    # Filter pairs where both the word and the tag match\n",
        "    word_tag_occurrences = [pair for pair in tag_occurrences if pair[0] == word]\n",
        "    count_word_given_tag = len(word_tag_occurrences)\n",
        "    \n",
        "    return (count_word_given_tag, count_tag)\n",
        "\n",
        "\n",
        "# Function to calculate the transition count from tag t1 to t2\n",
        "def tag_transition_counts(t1, t2, train_data=train_tagged_words):\n",
        "    # Extract only the sequence of tags from the training data\n",
        "    tag_sequence = [pair[1] for pair in train_data]\n",
        "    \n",
        "    # Count occurrences of the first tag (t1)\n",
        "    count_t1 = tag_sequence.count(t1)\n",
        "    \n",
        "    # Count occurrences of the transition t1 -> t2\n",
        "    count_t2_given_t1 = 0\n",
        "    for index in range(len(tag_sequence) - 1):\n",
        "        if tag_sequence[index] == t1 and tag_sequence[index+1] == t2:\n",
        "            count_t2_given_t1 += 1\n",
        "            \n",
        "    return (count_t2_given_t1, count_t1)\n",
        "\n",
        "\n",
        "# Calculate the Transition Probability Matrix P(t2 | t1)\n",
        "tag_list = list(pos_tags)\n",
        "tags_matrix = np.zeros((len(tag_list), len(tag_list)), dtype='float32')\n",
        "\n",
        "# Iterate through all possible tag transitions (t1 -> t2)\n",
        "for i, t1 in enumerate(tag_list):\n",
        "    for j, t2 in enumerate(tag_list):\n",
        "        count_t2_t1, count_t1 = tag_transition_counts(t1, t2)\n",
        "        \n",
        "        # Probability is P(t2|t1) = Count(t1, t2) / Count(t1)\n",
        "        tags_matrix[i, j] = count_t2_t1 / count_t1 if count_t1 != 0 else 0\n",
        "\n",
        "print(\"\\n--- Transition Probability Matrix (Numpy) ---\")\n",
        "print(tags_matrix)\n",
        "\n",
        "# Convert the transition matrix into a Pandas DataFrame for easier lookup (Transition Probabilities)\n",
        "tags_df = pd.DataFrame(tags_matrix, columns=tag_list, index=tag_list)\n",
        "\n",
        "print(\"\\n--- Transition Probability Matrix (DataFrame) ---\")\n",
        "print(tags_df)\n",
        "\n",
        "\n",
        "# Viterbi Algorithm implementation for POS Tagging\n",
        "def Viterbi_Tagger(word_sequence, train_data=train_tagged_words):\n",
        "    # The sequence of predicted tags will be stored here\n",
        "    predicted_state_sequence = []\n",
        "    # Get the set of all unique possible tags\n",
        "    all_tags = list(set([pair[1] for pair in train_data]))\n",
        "\n",
        "    # Iterate through the words in the input sequence\n",
        "    for index, current_word in enumerate(word_sequence):\n",
        "        probabilities = []\n",
        "        \n",
        "        # For each possible tag (t) for the current word\n",
        "        for current_tag in all_tags:\n",
        "            # 1. Calculate Transition Probability P(current_tag | previous_tag)\n",
        "            if index == 0:  # Start of sentence - P(current_tag | start_of_sentence)\n",
        "                # The tag '.' is used in this corpus as the end-of-sentence marker, \n",
        "                # which can be repurposed for the start state transition.\n",
        "                transition_prob = tags_df.loc['.', current_tag]\n",
        "            else:\n",
        "                # Transition from the last predicted tag to the current tag\n",
        "                transition_prob = tags_df.loc[predicted_state_sequence[-1], current_tag]\n",
        "\n",
        "            # 2. Calculate Emission Probability P(word | tag)\n",
        "            count_w_given_tag, count_tag = word_given_tag_prob(current_word, current_tag)\n",
        "            # Handle unknown words/tags gracefully by checking for count_tag != 0\n",
        "            emission_prob = count_w_given_tag / count_tag if count_tag != 0 else 0\n",
        "\n",
        "            # 3. Calculate the Viterbi probability\n",
        "            # Viterbi_Prob = Emission_Prob * Transition_Prob\n",
        "            state_probability = emission_prob * transition_prob\n",
        "            probabilities.append(state_probability)\n",
        "\n",
        "        # Find the maximum probability and the corresponding tag\n",
        "        max_prob = max(probabilities)\n",
        "        best_tag = all_tags[probabilities.index(max_prob)]\n",
        "        predicted_state_sequence.append(best_tag)\n",
        "\n",
        "    # Return the tagged sequence as a list of (word, tag) tuples\n",
        "    return list(zip(word_sequence, predicted_state_sequence))\n",
        "\n",
        "\n",
        "# Testing the Viterbi Tagger on a sample of the test set\n",
        "random.seed(1234)\n",
        "\n",
        "# Select 10 random sentence indices from the test set\n",
        "random_indices = [random.randint(0, len(test_set) - 1) for _ in range(10)]\n",
        "\n",
        "# Create the test run set using these random sentences\n",
        "test_sentences = [test_set[i] for i in random_indices]\n",
        "\n",
        "# Flatten the actual tagged words for comparison\n",
        "test_base_flat = [pair for sentence in test_sentences for pair in sentence]\n",
        "\n",
        "# Extract only the words to feed into the Viterbi Tagger\n",
        "input_words = [pair[0] for sentence in test_sentences for pair in sentence]\n",
        "\n",
        "print(f\"\\nTotal words in the test run sample: {len(input_words)}\")\n",
        "\n",
        "# Time and run the Viterbi Tagger\n",
        "start_time = time.time()\n",
        "predicted_sequence = Viterbi_Tagger(input_words)\n",
        "end_time = time.time()\n",
        "time_elapsed = end_time - start_time\n",
        "\n",
        "print(\"Time taken for Viterbi tagging (seconds): \", time_elapsed)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "# Compare the predicted (word, tag) pairs with the actual (word, tag) pairs\n",
        "correct_predictions = [1 for predicted, actual in zip(predicted_sequence, test_base_flat) if predicted == actual]\n",
        "\n",
        "accuracy = len(correct_predictions) / len(predicted_sequence)\n",
        "print('Viterbi HMM Tagger Accuracy on Sample: {:.2f}%'.format(accuracy * 100))\n",
        "\n",
        "# Display a sample result\n",
        "print(\"\\n--- Sample Predicted vs Actual Tagging ---\")\n",
        "for i in range(15):\n",
        "    print(f\"Word: {predicted_sequence[i][0]}, Predicted Tag: {predicted_sequence[i][1]}, Actual Tag: {test_base_flat[i][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2: Sentence Similarity Metrics (Jaccard, TF-IDF Cosine, Word2Vec Cosine)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Ensure the 'punkt' tokenizer is available for NLTK\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function for Jaccard Similarity (Set-based similarity)\n",
        "def calculate_jaccard_similarity(text_a, text_b):\n",
        "    # Convert to lowercase and split into words to create sets\n",
        "    set_a, set_b = set(text_a.lower().split()), set(text_b.lower().split())\n",
        "    # Jaccard = |Intersection| / |Union|\n",
        "    return len(set_a & set_b) / len(set_a | set_b)\n",
        "\n",
        "# Function for Cosine Similarity using TF-IDF vectors\n",
        "def calculate_tfidf_cosine_similarity(text_a, text_b):\n",
        "    # Initialize TF-IDF Vectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    # Fit and transform the two documents (sentences)\n",
        "    tfidf_vectors = vectorizer.fit_transform([text_a, text_b])\n",
        "    # Calculate cosine similarity between the two vectors\n",
        "    return cosine_similarity(tfidf_vectors[0:1], tfidf_vectors[1:2])[0][0]\n",
        "\n",
        "# Function for Cosine Similarity using Word2Vec sentence embeddings\n",
        "def calculate_word2vec_similarity(text_a, text_b):\n",
        "    # Tokenize sentences\n",
        "    tokens_a, tokens_b = word_tokenize(text_a.lower()), word_tokenize(text_b.lower())\n",
        "    \n",
        "    # Train a minimal Word2Vec model on the two sentences\n",
        "    # vector_size=100, window=3, min_count=1 are common starting parameters\n",
        "    word2vec_model = Word2Vec([tokens_a, tokens_b], vector_size=100, window=3, min_count=1, workers=1)\n",
        "    \n",
        "    # Calculate sentence vector by averaging word vectors (simple approach)\n",
        "    vec_a = sum(word2vec_model.wv[word] for word in tokens_a) / len(tokens_a)\n",
        "    vec_b = sum(word2vec_model.wv[word] for word in tokens_b) / len(tokens_b)\n",
        "    \n",
        "    # Compute cosine similarity between the two sentence vectors\n",
        "    return cosine_similarity([vec_a], [vec_b])[0][0]\n",
        "\n",
        "# Define the input sentences (CHANGED SAMPLE TEXTS)\n",
        "sentence_a = \"The ancient scroll describes a dragon that breathes fire and ice.\"\n",
        "sentence_b = \"An old parchment speaks of a serpent that exhales heat and frost.\"\n",
        "\n",
        "print(\"\\n--- Comparing Two Sentences ---\")\n",
        "print(f\"Sentence A: {sentence_a}\")\n",
        "print(f\"Sentence B: {sentence_b}\")\n",
        "\n",
        "# Calculate and display similarities\n",
        "print(\"\\nJaccard Similarity:\", calculate_jaccard_similarity(sentence_a, sentence_b))\n",
        "print(\"TF-IDF Cosine Similarity:\", calculate_tfidf_cosine_similarity(sentence_a, sentence_b))\n",
        "print(\"Word2Vec Similarity:\", calculate_word2vec_similarity(sentence_a, sentence_b))"
      ],
      "metadata": {
        "id": "9tFy7hFh2Evs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 3: N-Gram Language Model using Maximum Likelihood Estimation (MLE)\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.util import ngrams\n",
        "\n",
        "# Sample text corpus for training the language model (CHANGED SAMPLE TEXTS)\n",
        "sample_text = \"\"\"\n",
        "The cat sat on the mat.\n",
        "The dog ran past the cat.\n",
        "The mat is old.\n",
        "The dog is fast.\n",
        "The cat is black.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenization and lowercase conversion\n",
        "# Use regex to find word boundaries\n",
        "tokens = [word.lower() for word in re.findall(r\"\\b\\w+\\b\", sample_text)]\n",
        "print(f\"\\nTokens used for training: {tokens}\")\n",
        "\n",
        "print(\"\\n==========================\")\n",
        "\n",
        "## N=2 (Bigram) Language Model\n",
        "N_BIGRAM = 2\n",
        "# Prepare data for MLE model: padding sentences and generating N-grams\n",
        "train_data_bigram, vocab_bigram = padded_everygram_pipeline(N_BIGRAM, [tokens])\n",
        "\n",
        "# Initialize and fit the MLE model (no smoothing)\n",
        "lm_bigram = MLE(N_BIGRAM)\n",
        "lm_bigram.fit(train_data_bigram, vocab_bigram)\n",
        "\n",
        "print(f\"N={N_BIGRAM} (Bigram Model) Trained\")\n",
        "\n",
        "# Display a sample of bigram counts for inspection\n",
        "bigram_counts = {}\n",
        "for token in sorted(list(set(tokens))):\n",
        "    # Get counts of all words following the current token\n",
        "    bigram_counts[token] = dict(lm_bigram.counts[(token,)])\n",
        "print(\"Bigram successor counts (P(w_i | w_{i-1})):\", bigram_counts)\n",
        "\n",
        "print(\"\\n--- Word Prediction with Bigram Model ---\")\n",
        "# Generate a 3-word sequence starting from 'the'\n",
        "context_word = 'the'\n",
        "predicted_words_bigram = [context_word]\n",
        "\n",
        "for _ in range(3):\n",
        "    # Check if the context has any successors in the model\n",
        "    if lm_bigram.counts[(context_word,)]:\n",
        "        # Find the word with the maximum count (most probable next word)\n",
        "        next_word = max(lm_bigram.counts[(context_word,)].items(), key=lambda x: x[1])[0]\n",
        "        predicted_words_bigram.append(next_word)\n",
        "        context_word = next_word # Update context for next iteration\n",
        "    else:\n",
        "        break\n",
        "print(f\"Bigram prediction (start='the', max 3 words): {' '.join(predicted_words_bigram)}\")\n",
        "\n",
        "print(\"\\n==========================\")\n",
        "\n",
        "## N=3 (Trigram) Language Model\n",
        "N_TRIGRAM = 3\n",
        "train_data_trigram, vocab_trigram = padded_everygram_pipeline(N_TRIGRAM, [tokens])\n",
        "\n",
        "lm_trigram = MLE(N_TRIGRAM)\n",
        "lm_trigram.fit(train_data_trigram, vocab_trigram)\n",
        "\n",
        "print(f\"N={N_TRIGRAM} (Trigram Model) Trained\")\n",
        "\n",
        "# Display a sample of trigram counts\n",
        "trigram_counts = {}\n",
        "bigrams_in_text = list(ngrams(tokens, N_TRIGRAM - 1)) # Get all bigram contexts\n",
        "for bigram_context in sorted(list(set(bigrams_in_text))):\n",
        "    # Get counts of all words following the bigram context\n",
        "    trigram_counts[bigram_context] = dict(lm_trigram.counts[bigram_context])\n",
        "print(\"Trigram successor counts (P(w_i | w_{i-2} w_{i-1})):\", trigram_counts)\n",
        "\n",
        "print(\"\\n--- Word Prediction with Trigram Model ---\")\n",
        "# Generate a 3-word sequence starting from 'the cat'\n",
        "context_list = ['the', 'cat'] # Need two words for a trigram context\n",
        "predicted_words_trigram = context_list.copy()\n",
        "\n",
        "for _ in range(3):\n",
        "    context_tuple = tuple(context_list)\n",
        "    # Check if the bigram context has any successors\n",
        "    if lm_trigram.counts[context_tuple]:\n",
        "        # Find the word with the maximum count\n",
        "        next_word = max(lm_trigram.counts[context_tuple].items(), key=lambda x: x[1])[0]\n",
        "        predicted_words_trigram.append(next_word)\n",
        "        # Update the context to the last two words\n",
        "        context_list = [context_list[-1], next_word]\n",
        "    else:\n",
        "        break\n",
        "print(f\"Trigram prediction (start='the cat', max 3 words): {' '.join(predicted_words_trigram)}\")"
      ],
      "metadata": {
        "id": "k4iF-P4_2HKc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
