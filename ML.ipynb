{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvvUWvU9o9O22HLhXam2sL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kryptobolt07/Paytm/blob/main/Ml%20programs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_36PV0q19V0"
      },
      "outputs": [],
      "source": [
        "# Part 1: Hidden Markov Model (HMM) for Part-of-Speech Tagging using Viterbi Algorithm\n",
        "# Note: This section uses the NLTK Treebank corpus, so the \"sample text\" for the model itself is fixed, \n",
        "# but the test sample sentences are randomized.\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('treebank')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "# Get the Penn Treebank corpus tagged with Universal Tagset\n",
        "corpus_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n",
        "\n",
        "print(\"\\n--- Sample Tagged Sentences from Treebank ---\")\n",
        "print(corpus_data[:2])\n",
        "\n",
        "# Display the word/tag tuples individually for better inspection\n",
        "print(\"\\n--- Individual Word/Tag Pairs for First Two Sentences ---\")\n",
        "for sentence in corpus_data[:2]:\n",
        "    for word_tag_pair in sentence:\n",
        "        print(word_tag_pair)\n",
        "\n",
        "# Split the corpus into training and testing sets\n",
        "train_set, test_set = train_test_split(corpus_data, train_size=0.80, test_size=0.20, random_state=101)\n",
        "\n",
        "# Flatten the list of sentences into a single list of (word, tag) tuples for both sets\n",
        "train_tagged_words = [pair for sentence in train_set for pair in sentence]\n",
        "test_tagged_words_flat = [pair for sentence in test_set for pair in sentence] # Renamed to avoid confusion with words-only list later\n",
        "\n",
        "print(f\"\\nTotal tagged words in training data: {len(train_tagged_words)}\")\n",
        "print(f\"Total tagged words in testing data: {len(test_tagged_words_flat)}\")\n",
        "\n",
        "# Check the first few tagged words in the training data\n",
        "print(f\"\\nFirst 5 tagged words in training data: {train_tagged_words[:5]}\")\n",
        "\n",
        "# Determine the unique set of POS tags in the training data\n",
        "pos_tags = {tag for word, tag in train_tagged_words}\n",
        "print(f\"\\nNumber of unique tags: {len(pos_tags)}\")\n",
        "print(f\"Unique POS tags: {pos_tags}\")\n",
        "\n",
        "# Determine the total unique words (vocabulary) in the training data\n",
        "vocabulary = {word for word, tag in train_tagged_words}\n",
        "\n",
        "print(f\"Total unique words in vocabulary: {len(vocabulary)}\")\n",
        "\n",
        "\n",
        "# Function to calculate the probability of a word given a tag (Emission Probability P(word | tag))\n",
        "def word_given_tag_prob(word, tag, train_data=train_tagged_words):\n",
        "    # Filter pairs matching the given tag\n",
        "    tag_occurrences = [pair for pair in train_data if pair[1] == tag]\n",
        "    count_tag = len(tag_occurrences) # Count of the specific tag\n",
        "    \n",
        "    # Filter pairs where both the word and the tag match\n",
        "    word_tag_occurrences = [pair for pair in tag_occurrences if pair[0] == word]\n",
        "    count_word_given_tag = len(word_tag_occurrences)\n",
        "    \n",
        "    return (count_word_given_tag, count_tag)\n",
        "\n",
        "\n",
        "# Function to calculate the transition count from tag t1 to t2\n",
        "def tag_transition_counts(t1, t2, train_data=train_tagged_words):\n",
        "    # Extract only the sequence of tags from the training data\n",
        "    tag_sequence = [pair[1] for pair in train_data]\n",
        "    \n",
        "    # Count occurrences of the first tag (t1)\n",
        "    count_t1 = tag_sequence.count(t1)\n",
        "    \n",
        "    # Count occurrences of the transition t1 -> t2\n",
        "    count_t2_given_t1 = 0\n",
        "    for index in range(len(tag_sequence) - 1):\n",
        "        if tag_sequence[index] == t1 and tag_sequence[index+1] == t2:\n",
        "            count_t2_given_t1 += 1\n",
        "            \n",
        "    return (count_t2_given_t1, count_t1)\n",
        "\n",
        "\n",
        "# Calculate the Transition Probability Matrix P(t2 | t1)\n",
        "tag_list = list(pos_tags)\n",
        "tags_matrix = np.zeros((len(tag_list), len(tag_list)), dtype='float32')\n",
        "\n",
        "# Iterate through all possible tag transitions (t1 -> t2)\n",
        "for i, t1 in enumerate(tag_list):\n",
        "    for j, t2 in enumerate(tag_list):\n",
        "        count_t2_t1, count_t1 = tag_transition_counts(t1, t2)\n",
        "        \n",
        "        # Probability is P(t2|t1) = Count(t1, t2) / Count(t1)\n",
        "        tags_matrix[i, j] = count_t2_t1 / count_t1 if count_t1 != 0 else 0\n",
        "\n",
        "print(\"\\n--- Transition Probability Matrix (Numpy) ---\")\n",
        "print(tags_matrix)\n",
        "\n",
        "# Convert the transition matrix into a Pandas DataFrame for easier lookup (Transition Probabilities)\n",
        "tags_df = pd.DataFrame(tags_matrix, columns=tag_list, index=tag_list)\n",
        "\n",
        "print(\"\\n--- Transition Probability Matrix (DataFrame) ---\")\n",
        "print(tags_df)\n",
        "\n",
        "\n",
        "# Viterbi Algorithm implementation for POS Tagging\n",
        "def Viterbi_Tagger(word_sequence, train_data=train_tagged_words):\n",
        "    # The sequence of predicted tags will be stored here\n",
        "    predicted_state_sequence = []\n",
        "    # Get the set of all unique possible tags\n",
        "    all_tags = list(set([pair[1] for pair in train_data]))\n",
        "\n",
        "    # Iterate through the words in the input sequence\n",
        "    for index, current_word in enumerate(word_sequence):\n",
        "        probabilities = []\n",
        "        \n",
        "        # For each possible tag (t) for the current word\n",
        "        for current_tag in all_tags:\n",
        "            # 1. Calculate Transition Probability P(current_tag | previous_tag)\n",
        "            if index == 0:  # Start of sentence - P(current_tag | start_of_sentence)\n",
        "                # The tag '.' is used in this corpus as the end-of-sentence marker, \n",
        "                # which can be repurposed for the start state transition.\n",
        "                transition_prob = tags_df.loc['.', current_tag]\n",
        "            else:\n",
        "                # Transition from the last predicted tag to the current tag\n",
        "                transition_prob = tags_df.loc[predicted_state_sequence[-1], current_tag]\n",
        "\n",
        "            # 2. Calculate Emission Probability P(word | tag)\n",
        "            count_w_given_tag, count_tag = word_given_tag_prob(current_word, current_tag)\n",
        "            # Handle unknown words/tags gracefully by checking for count_tag != 0\n",
        "            emission_prob = count_w_given_tag / count_tag if count_tag != 0 else 0\n",
        "\n",
        "            # 3. Calculate the Viterbi probability\n",
        "            # Viterbi_Prob = Emission_Prob * Transition_Prob\n",
        "            state_probability = emission_prob * transition_prob\n",
        "            probabilities.append(state_probability)\n",
        "\n",
        "        # Find the maximum probability and the corresponding tag\n",
        "        max_prob = max(probabilities)\n",
        "        best_tag = all_tags[probabilities.index(max_prob)]\n",
        "        predicted_state_sequence.append(best_tag)\n",
        "\n",
        "    # Return the tagged sequence as a list of (word, tag) tuples\n",
        "    return list(zip(word_sequence, predicted_state_sequence))\n",
        "\n",
        "\n",
        "# Testing the Viterbi Tagger on a sample of the test set\n",
        "random.seed(1234)\n",
        "\n",
        "# Select 10 random sentence indices from the test set\n",
        "random_indices = [random.randint(0, len(test_set) - 1) for _ in range(10)]\n",
        "\n",
        "# Create the test run set using these random sentences\n",
        "test_sentences = [test_set[i] for i in random_indices]\n",
        "\n",
        "# Flatten the actual tagged words for comparison\n",
        "test_base_flat = [pair for sentence in test_sentences for pair in sentence]\n",
        "\n",
        "# Extract only the words to feed into the Viterbi Tagger\n",
        "input_words = [pair[0] for sentence in test_sentences for pair in sentence]\n",
        "\n",
        "print(f\"\\nTotal words in the test run sample: {len(input_words)}\")\n",
        "\n",
        "# Time and run the Viterbi Tagger\n",
        "start_time = time.time()\n",
        "predicted_sequence = Viterbi_Tagger(input_words)\n",
        "end_time = time.time()\n",
        "time_elapsed = end_time - start_time\n",
        "\n",
        "print(\"Time taken for Viterbi tagging (seconds): \", time_elapsed)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "# Compare the predicted (word, tag) pairs with the actual (word, tag) pairs\n",
        "correct_predictions = [1 for predicted, actual in zip(predicted_sequence, test_base_flat) if predicted == actual]\n",
        "\n",
        "accuracy = len(correct_predictions) / len(predicted_sequence)\n",
        "print('Viterbi HMM Tagger Accuracy on Sample: {:.2f}%'.format(accuracy * 100))\n",
        "\n",
        "# Display a sample result\n",
        "print(\"\\n--- Sample Predicted vs Actual Tagging ---\")\n",
        "for i in range(15):\n",
        "    print(f\"Word: {predicted_sequence[i][0]}, Predicted Tag: {predicted_sequence[i][1]}, Actual Tag: {test_base_flat[i][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2: Sentence Similarity Metrics (Jaccard, TF-IDF Cosine, Word2Vec Cosine)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Ensure the 'punkt' tokenizer is available for NLTK\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function for Jaccard Similarity (Set-based similarity)\n",
        "def calculate_jaccard_similarity(text_a, text_b):\n",
        "    # Convert to lowercase and split into words to create sets\n",
        "    set_a, set_b = set(text_a.lower().split()), set(text_b.lower().split())\n",
        "    # Jaccard = |Intersection| / |Union|\n",
        "    return len(set_a & set_b) / len(set_a | set_b)\n",
        "\n",
        "# Function for Cosine Similarity using TF-IDF vectors\n",
        "def calculate_tfidf_cosine_similarity(text_a, text_b):\n",
        "    # Initialize TF-IDF Vectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    # Fit and transform the two documents (sentences)\n",
        "    tfidf_vectors = vectorizer.fit_transform([text_a, text_b])\n",
        "    # Calculate cosine similarity between the two vectors\n",
        "    return cosine_similarity(tfidf_vectors[0:1], tfidf_vectors[1:2])[0][0]\n",
        "\n",
        "# Function for Cosine Similarity using Word2Vec sentence embeddings\n",
        "def calculate_word2vec_similarity(text_a, text_b):\n",
        "    # Tokenize sentences\n",
        "    tokens_a, tokens_b = word_tokenize(text_a.lower()), word_tokenize(text_b.lower())\n",
        "    \n",
        "    # Train a minimal Word2Vec model on the two sentences\n",
        "    # vector_size=100, window=3, min_count=1 are common starting parameters\n",
        "    word2vec_model = Word2Vec([tokens_a, tokens_b], vector_size=100, window=3, min_count=1, workers=1)\n",
        "    \n",
        "    # Calculate sentence vector by averaging word vectors (simple approach)\n",
        "    vec_a = sum(word2vec_model.wv[word] for word in tokens_a) / len(tokens_a)\n",
        "    vec_b = sum(word2vec_model.wv[word] for word in tokens_b) / len(tokens_b)\n",
        "    \n",
        "    # Compute cosine similarity between the two sentence vectors\n",
        "    return cosine_similarity([vec_a], [vec_b])[0][0]\n",
        "\n",
        "# Define the input sentences (CHANGED SAMPLE TEXTS)\n",
        "sentence_a = \"The ancient scroll describes a dragon that breathes fire and ice.\"\n",
        "sentence_b = \"An old parchment speaks of a serpent that exhales heat and frost.\"\n",
        "\n",
        "print(\"\\n--- Comparing Two Sentences ---\")\n",
        "print(f\"Sentence A: {sentence_a}\")\n",
        "print(f\"Sentence B: {sentence_b}\")\n",
        "\n",
        "# Calculate and display similarities\n",
        "print(\"\\nJaccard Similarity:\", calculate_jaccard_similarity(sentence_a, sentence_b))\n",
        "print(\"TF-IDF Cosine Similarity:\", calculate_tfidf_cosine_similarity(sentence_a, sentence_b))\n",
        "print(\"Word2Vec Similarity:\", calculate_word2vec_similarity(sentence_a, sentence_b))"
      ],
      "metadata": {
        "id": "9tFy7hFh2Evs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 3: N-Gram Language Model using Maximum Likelihood Estimation (MLE)\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.util import ngrams\n",
        "\n",
        "# Sample text corpus for training the language model (CHANGED SAMPLE TEXTS)\n",
        "sample_text = \"\"\"\n",
        "The cat sat on the mat.\n",
        "The dog ran past the cat.\n",
        "The mat is old.\n",
        "The dog is fast.\n",
        "The cat is black.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenization and lowercase conversion\n",
        "# Use regex to find word boundaries\n",
        "tokens = [word.lower() for word in re.findall(r\"\\b\\w+\\b\", sample_text)]\n",
        "print(f\"\\nTokens used for training: {tokens}\")\n",
        "\n",
        "print(\"\\n==========================\")\n",
        "\n",
        "## N=2 (Bigram) Language Model\n",
        "N_BIGRAM = 2\n",
        "# Prepare data for MLE model: padding sentences and generating N-grams\n",
        "train_data_bigram, vocab_bigram = padded_everygram_pipeline(N_BIGRAM, [tokens])\n",
        "\n",
        "# Initialize and fit the MLE model (no smoothing)\n",
        "lm_bigram = MLE(N_BIGRAM)\n",
        "lm_bigram.fit(train_data_bigram, vocab_bigram)\n",
        "\n",
        "print(f\"N={N_BIGRAM} (Bigram Model) Trained\")\n",
        "\n",
        "# Display a sample of bigram counts for inspection\n",
        "bigram_counts = {}\n",
        "for token in sorted(list(set(tokens))):\n",
        "    # Get counts of all words following the current token\n",
        "    bigram_counts[token] = dict(lm_bigram.counts[(token,)])\n",
        "print(\"Bigram successor counts (P(w_i | w_{i-1})):\", bigram_counts)\n",
        "\n",
        "print(\"\\n--- Word Prediction with Bigram Model ---\")\n",
        "# Generate a 3-word sequence starting from 'the'\n",
        "context_word = 'the'\n",
        "predicted_words_bigram = [context_word]\n",
        "\n",
        "for _ in range(3):\n",
        "    # Check if the context has any successors in the model\n",
        "    if lm_bigram.counts[(context_word,)]:\n",
        "        # Find the word with the maximum count (most probable next word)\n",
        "        next_word = max(lm_bigram.counts[(context_word,)].items(), key=lambda x: x[1])[0]\n",
        "        predicted_words_bigram.append(next_word)\n",
        "        context_word = next_word # Update context for next iteration\n",
        "    else:\n",
        "        break\n",
        "print(f\"Bigram prediction (start='the', max 3 words): {' '.join(predicted_words_bigram)}\")\n",
        "\n",
        "print(\"\\n==========================\")\n",
        "\n",
        "## N=3 (Trigram) Language Model\n",
        "N_TRIGRAM = 3\n",
        "train_data_trigram, vocab_trigram = padded_everygram_pipeline(N_TRIGRAM, [tokens])\n",
        "\n",
        "lm_trigram = MLE(N_TRIGRAM)\n",
        "lm_trigram.fit(train_data_trigram, vocab_trigram)\n",
        "\n",
        "print(f\"N={N_TRIGRAM} (Trigram Model) Trained\")\n",
        "\n",
        "# Display a sample of trigram counts\n",
        "trigram_counts = {}\n",
        "bigrams_in_text = list(ngrams(tokens, N_TRIGRAM - 1)) # Get all bigram contexts\n",
        "for bigram_context in sorted(list(set(bigrams_in_text))):\n",
        "    # Get counts of all words following the bigram context\n",
        "    trigram_counts[bigram_context] = dict(lm_trigram.counts[bigram_context])\n",
        "print(\"Trigram successor counts (P(w_i | w_{i-2} w_{i-1})):\", trigram_counts)\n",
        "\n",
        "print(\"\\n--- Word Prediction with Trigram Model ---\")\n",
        "# Generate a 3-word sequence starting from 'the cat'\n",
        "context_list = ['the', 'cat'] # Need two words for a trigram context\n",
        "predicted_words_trigram = context_list.copy()\n",
        "\n",
        "for _ in range(3):\n",
        "    context_tuple = tuple(context_list)\n",
        "    # Check if the bigram context has any successors\n",
        "    if lm_trigram.counts[context_tuple]:\n",
        "        # Find the word with the maximum count\n",
        "        next_word = max(lm_trigram.counts[context_tuple].items(), key=lambda x: x[1])[0]\n",
        "        predicted_words_trigram.append(next_word)\n",
        "        # Update the context to the last two words\n",
        "        context_list = [context_list[-1], next_word]\n",
        "    else:\n",
        "        break\n",
        "print(f\"Trigram prediction (start='the cat', max 3 words): {' '.join(predicted_words_trigram)}\")"
      ],
      "metadata": {
        "id": "k4iF-P4_2HKc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
