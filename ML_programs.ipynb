{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/ZHkebyMK9ogQZqGMY7f+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kryptobolt07/Paytm/blob/main/ML_programs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iPamMwqTcZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0dd6f25-a127-41ce-a79e-6568e675dbcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "tokens: ['Apple', 'released', 'a', 'new', 'iPhone', 'in', 'California', 'and', 'many', 'people', 'bought', 'it', 'quickly', '.']\n",
            "no-stop: ['Apple', 'released', 'new', 'iPhone', 'California', 'many', 'people', 'bought', 'quickly']\n",
            "stems: ['appl', 'releas', 'a', 'new', 'iphon', 'in', 'california', 'and', 'mani', 'peopl', 'bought', 'it', 'quickli', '.']\n",
            "lemmas_nltk: ['Apple', 'released', 'a', 'new', 'iPhone', 'in', 'California', 'and', 'many', 'people', 'bought', 'it', 'quickly', '.']\n",
            "tokens_spacy: ['Apple', 'released', 'a', 'new', 'iPhone', 'in', 'California', 'and', 'many', 'people', 'bought', 'it', 'quickly', '.']\n",
            "lemmas_spacy: ['Apple', 'release', 'a', 'new', 'iPhone', 'in', 'California', 'and', 'many', 'people', 'buy', 'it', 'quickly', '.']\n"
          ]
        }
      ],
      "source": [
        "# Program 1: Preprocessing (tokenize, stopwords, stem, lemma)\n",
        "import nltk, spacy\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download(\"punkt_tab\"); nltk.download(\"stopwords\"); nltk.download(\"wordnet\"); nltk.download(\"omw-1.4\")\n",
        "spacy.cli.download(\"en_core_web_sm\"); nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Apple released a new iPhone in California and many people bought it quickly.\"\n",
        "tokens = word_tokenize(text)\n",
        "stopset = set(stopwords.words(\"english\"))\n",
        "tokens_no_stop = [t for t in tokens if t.isalnum() and t.lower() not in stopset]\n",
        "pst = PorterStemmer(); wnl = WordNetLemmatizer()\n",
        "stems = [pst.stem(t) for t in tokens]\n",
        "lemmas_nltk = [wnl.lemmatize(t) for t in tokens]\n",
        "doc = nlp(text)\n",
        "tokens_spacy = [t.text for t in doc]; lemmas_spacy = [t.lemma_ for t in doc]\n",
        "\n",
        "print(\"tokens:\", tokens)\n",
        "print(\"no-stop:\", tokens_no_stop)\n",
        "print(\"stems:\", stems)\n",
        "print(\"lemmas_nltk:\", lemmas_nltk)\n",
        "print(\"tokens_spacy:\", tokens_spacy)\n",
        "print(\"lemmas_spacy:\", lemmas_spacy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 2: Named Entity Recognition\n",
        "import nltk, spacy\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk import Tree\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
        "nltk.download(\"maxent_ne_chunker\")\n",
        "nltk.download(\"maxent_ne_chunker_tab\")\n",
        "nltk.download(\"words\")\n",
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Aditya spoke at Microsoft in Goregoan on January 18, 2015.\"\n",
        "# spaCy\n",
        "doc = nlp(text)\n",
        "print(\"spaCy:\", [(e.text, e.label_) for e in doc.ents])\n",
        "# NLTK\n",
        "toks = word_tokenize(text)\n",
        "pos = pos_tag(toks)\n",
        "tree = ne_chunk(pos)\n",
        "nltk_entities = []\n",
        "for node in tree:\n",
        "    if isinstance(node, Tree):\n",
        "        name = \" \".join(w for w,t in node.leaves())\n",
        "        label = node.label()\n",
        "        nltk_entities.append((name, label))\n",
        "print(\"NLTK:\", nltk_entities)"
      ],
      "metadata": {
        "id": "ajm_gk71ToUx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e40f70-fbfc-4abe-8418-7f086d3561c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "spaCy: [('Aditya', 'PERSON'), ('Microsoft', 'ORG'), ('Goregoan', 'GPE'), ('January 18, 2015', 'DATE')]\n",
            "NLTK: [('Aditya', 'PERSON'), ('Microsoft', 'ORGANIZATION'), ('Goregoan', 'GPE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Experiment 4\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, RegexpParser\n",
        "\n",
        "text = \"\"\"Prime Minister Narendra Modi on Tuesday announced free smartphones for the entire population of India to fight against coronavirus pandemic\"\"\"\n",
        "\n",
        "words = word_tokenize(text)\n",
        "postags = pos_tag(words)\n",
        "print(\"POS Tags:\")\n",
        "print(postags)\n",
        "\n",
        "grammar = r\"\"\"\n",
        "  NP: {<DT>?<JJ>*<NN.*>+}\n",
        "\"\"\"\n",
        "\n",
        "chunk_parser = RegexpParser(grammar)\n",
        "\n",
        "tree = chunk_parser.parse(postags)\n",
        "\n",
        "print(\"\\nChunked Tree:\")\n",
        "print(tree)"
      ],
      "metadata": {
        "id": "uZlH48yqTux0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea46c31d-f18d-4a1e-9385-2109f67a3949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags:\n",
            "[('Prime', 'NNP'), ('Minister', 'NNP'), ('Narendra', 'NNP'), ('Modi', 'NNP'), ('on', 'IN'), ('Tuesday', 'NNP'), ('announced', 'VBD'), ('free', 'JJ'), ('smartphones', 'NNS'), ('for', 'IN'), ('the', 'DT'), ('entire', 'JJ'), ('population', 'NN'), ('of', 'IN'), ('India', 'NNP'), ('to', 'TO'), ('fight', 'VB'), ('against', 'IN'), ('coronavirus', 'NN'), ('pandemic', 'NN')]\n",
            "\n",
            "Chunked Tree:\n",
            "(S\n",
            "  (NP Prime/NNP Minister/NNP Narendra/NNP Modi/NNP)\n",
            "  on/IN\n",
            "  (NP Tuesday/NNP)\n",
            "  announced/VBD\n",
            "  (NP free/JJ smartphones/NNS)\n",
            "  for/IN\n",
            "  (NP the/DT entire/JJ population/NN)\n",
            "  of/IN\n",
            "  (NP India/NNP)\n",
            "  to/TO\n",
            "  fight/VB\n",
            "  against/IN\n",
            "  (NP coronavirus/NN pandemic/NN))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pprint, time\n",
        "\n",
        "\n",
        "nltk.download('treebank')\n",
        "\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n",
        "\n",
        "\n",
        "print(nltk_data[:2])\n",
        "\n",
        "for sent in nltk_data[:2]:\n",
        "    for item in sent:\n",
        "        print(item)\n",
        "\n",
        "\n",
        "train_set, test_set =train_test_split(nltk_data,train_size=0.80,test_size=0.20,random_state = 101)\n",
        "\n",
        "\n",
        "train_tagged_words = [tup for sent in train_set for tup in sent ]\n",
        "test_tagged_words = [tup for sent in test_set for tup in sent ]\n",
        "print(len(train_tagged_words))\n",
        "print(len(test_tagged_words))\n",
        "\n",
        "# check some of the tagged words.\n",
        "print(train_tagged_words[:5])\n",
        "\n",
        "# use set datatype to check how many unique tags are present in training data\n",
        "tags = {tag for word,tag in train_tagged_words}\n",
        "print(len(tags))\n",
        "print(tags)\n",
        "\n",
        "# check total words in vocabulary\n",
        "vocab = {word for word,tag in train_tagged_words}\n",
        "\n",
        "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
        "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
        "    count_tag = len(tag_list) #total number of times the passed tag occurred in train_bag\n",
        "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
        "\n",
        "    count_w_given_tag = len(w_given_tag_list)\n",
        "\n",
        "    return (count_w_given_tag, count_tag)\n",
        "\n",
        "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
        "    tags_list = [pair[1] for pair in train_bag]\n",
        "    count_t1 = len([t for t in tags_list if t==t1])\n",
        "    count_t2_t1 = 0\n",
        "    for index in range(len(tags_list)-1):\n",
        "        if tags_list[index] == t1 and tags_list[index+1] == t2:\n",
        "            count_t2_t1 += 1\n",
        "    return (count_t2_t1, count_t1)\n",
        "\n",
        "\n",
        "tags_list = list(tags)\n",
        "tags_matrix = np.zeros((len(tags_list), len(tags_list)), dtype='float32')\n",
        "for i, t1 in enumerate(tags_list):\n",
        "    for j, t2 in enumerate(tags_list):\n",
        "        count_t2_t1, count_t1 = t2_given_t1(t2, t1)\n",
        "        tags_matrix[i, j] = count_t2_t1 / count_t1 if count_t1 != 0 else 0\n",
        "\n",
        "print(tags_matrix)\n",
        "\n",
        "\n",
        "tags_df = pd.DataFrame(tags_matrix, columns = list(tags), index=list(tags))\n",
        "\n",
        "print(tags_df)\n",
        "\n",
        "def Viterbi(words, train_bag = train_tagged_words):\n",
        "    state = []\n",
        "    T = list(set([pair[1] for pair in train_bag]))\n",
        "\n",
        "    for key, word in enumerate(words):\n",
        "        p = []\n",
        "        for tag in T:\n",
        "            if key == 0:\n",
        "                transition_p = tags_df.loc['.', tag]\n",
        "            else:\n",
        "                transition_p = tags_df.loc[state[-1], tag]\n",
        "\n",
        "            count_w_given_tag, count_tag = word_given_tag(word, tag)\n",
        "            # Added check for division by zero\n",
        "            emission_p = count_w_given_tag / count_tag if count_tag != 0 else 0\n",
        "\n",
        "            state_probability = emission_p * transition_p\n",
        "            p.append(state_probability)\n",
        "\n",
        "        pmax = max(p)\n",
        "        state_max = T[p.index(pmax)]\n",
        "        state.append(state_max)\n",
        "\n",
        "    return list(zip(words, state))\n",
        "\n",
        "\n",
        "random.seed(1234)\n",
        "\n",
        "\n",
        "rnom = [random.randint(0,len(test_set) - 1) for x in range(10)] #\n",
        "\n",
        "\n",
        "test_run = [test_set[i] for i in rnom]\n",
        "\n",
        "test_run_base = [tup for sent in test_run for tup in sent]\n",
        "\n",
        "test_tagged_words = [tup[0] for sent in test_run for tup in sent]\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "tagged_seq = Viterbi(test_tagged_words)\n",
        "end = time.time()\n",
        "difference = end-start\n",
        "\n",
        "print(\"Time taken in seconds: \", difference)\n",
        "\n",
        "\n",
        "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j]\n",
        "\n",
        "accuracy = len(check)/len(tagged_seq)\n",
        "print('Viterbi Algorithm Accuracy: ',accuracy*100)"
      ],
      "metadata": {
        "id": "ZLJccE0gT3TM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ca33181-a18e-4852-d7ab-f23d192d6d51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')]]\n",
            "('Pierre', 'NOUN')\n",
            "('Vinken', 'NOUN')\n",
            "(',', '.')\n",
            "('61', 'NUM')\n",
            "('years', 'NOUN')\n",
            "('old', 'ADJ')\n",
            "(',', '.')\n",
            "('will', 'VERB')\n",
            "('join', 'VERB')\n",
            "('the', 'DET')\n",
            "('board', 'NOUN')\n",
            "('as', 'ADP')\n",
            "('a', 'DET')\n",
            "('nonexecutive', 'ADJ')\n",
            "('director', 'NOUN')\n",
            "('Nov.', 'NOUN')\n",
            "('29', 'NUM')\n",
            "('.', '.')\n",
            "('Mr.', 'NOUN')\n",
            "('Vinken', 'NOUN')\n",
            "('is', 'VERB')\n",
            "('chairman', 'NOUN')\n",
            "('of', 'ADP')\n",
            "('Elsevier', 'NOUN')\n",
            "('N.V.', 'NOUN')\n",
            "(',', '.')\n",
            "('the', 'DET')\n",
            "('Dutch', 'NOUN')\n",
            "('publishing', 'VERB')\n",
            "('group', 'NOUN')\n",
            "('.', '.')\n",
            "80310\n",
            "20366\n",
            "[('Drink', 'NOUN'), ('Carrier', 'NOUN'), ('Competes', 'VERB'), ('With', 'ADP'), ('Cartons', 'NOUN')]\n",
            "12\n",
            "{'ADP', 'VERB', 'DET', 'CONJ', '.', 'PRON', 'ADJ', 'X', 'NUM', 'PRT', 'NOUN', 'ADV'}\n",
            "[[1.69577319e-02 8.47886596e-03 3.20931405e-01 1.01240189e-03\n",
            "  3.87243740e-02 6.96026310e-02 1.07061505e-01 3.45482156e-02\n",
            "  6.32751212e-02 1.26550242e-03 3.23588967e-01 1.45532778e-02]\n",
            " [9.23572779e-02 1.67955801e-01 1.33609578e-01 5.43278083e-03\n",
            "  3.48066315e-02 3.55432779e-02 6.63904250e-02 2.15930015e-01\n",
            "  2.28360966e-02 3.06629837e-02 1.10589318e-01 8.38858187e-02]\n",
            " [9.91806854e-03 4.02472317e-02 6.03708485e-03 4.31220367e-04\n",
            "  1.73925534e-02 3.30602261e-03 2.06410810e-01 4.51343954e-02\n",
            "  2.28546783e-02 2.87480245e-04 6.35906279e-01 1.20741697e-02]\n",
            " [5.59824370e-02 1.50384188e-01 1.23490669e-01 5.48847427e-04\n",
            "  3.51262353e-02 6.03732169e-02 1.13611415e-01 9.33040585e-03\n",
            "  4.06147093e-02 4.39077942e-03 3.49066973e-01 5.70801310e-02]\n",
            " [9.29084867e-02 8.96899477e-02 1.72191828e-01 6.00793920e-02\n",
            "  9.23720598e-02 6.87694475e-02 4.61323895e-02 2.56410260e-02\n",
            "  7.82104954e-02 2.78940029e-03 2.18538776e-01 5.25694676e-02]\n",
            " [2.23234631e-02 4.84738052e-01 9.56719834e-03 5.01138950e-03\n",
            "  4.19134386e-02 6.83371304e-03 7.06150308e-02 8.83826911e-02\n",
            "  6.83371304e-03 1.41230067e-02 2.12756261e-01 3.69020514e-02]\n",
            " [8.05825219e-02 1.14563107e-02 5.24271838e-03 1.68932043e-02\n",
            "  6.60194159e-02 1.94174761e-04 6.33009672e-02 2.09708735e-02\n",
            "  2.17475723e-02 1.14563107e-02 6.96893215e-01 5.24271838e-03]\n",
            " [1.42225638e-01 2.06419379e-01 5.68902567e-02 1.03786280e-02\n",
            "  1.60868734e-01 5.41995019e-02 1.76821072e-02 7.57255405e-02\n",
            "  3.07514891e-03 1.85085520e-01 6.16951771e-02 2.57543717e-02]\n",
            " [3.74866128e-02 2.07068902e-02 3.57015361e-03 1.42806144e-02\n",
            "  1.19243130e-01 1.42806140e-03 3.53445187e-02 2.02427700e-01\n",
            "  1.84219927e-01 2.60621198e-02 3.51660132e-01 3.57015361e-03]\n",
            " [1.95694715e-02 4.01174158e-01 1.01369865e-01 2.34833662e-03\n",
            "  4.50097844e-02 1.76125243e-02 8.29745606e-02 1.21330721e-02\n",
            "  5.67514673e-02 1.17416831e-03 2.50489235e-01 9.39334650e-03]\n",
            " [1.76826611e-01 1.49133503e-01 1.31063312e-02 4.24540639e-02\n",
            "  2.40094051e-01 4.65906132e-03 1.25838192e-02 2.88252197e-02\n",
            "  9.14395228e-03 4.39345129e-02 2.62344331e-01 1.68945398e-02]\n",
            " [1.19472459e-01 3.39022487e-01 7.13731572e-02 6.98215654e-03\n",
            "  1.39255241e-01 1.20248254e-02 1.30721495e-01 2.28859577e-02\n",
            "  2.98681147e-02 1.47401085e-02 3.21955010e-02 8.14584941e-02]]\n",
            "           ADP      VERB       DET      CONJ         .      PRON       ADJ  \\\n",
            "ADP   0.016958  0.008479  0.320931  0.001012  0.038724  0.069603  0.107062   \n",
            "VERB  0.092357  0.167956  0.133610  0.005433  0.034807  0.035543  0.066390   \n",
            "DET   0.009918  0.040247  0.006037  0.000431  0.017393  0.003306  0.206411   \n",
            "CONJ  0.055982  0.150384  0.123491  0.000549  0.035126  0.060373  0.113611   \n",
            ".     0.092908  0.089690  0.172192  0.060079  0.092372  0.068769  0.046132   \n",
            "PRON  0.022323  0.484738  0.009567  0.005011  0.041913  0.006834  0.070615   \n",
            "ADJ   0.080583  0.011456  0.005243  0.016893  0.066019  0.000194  0.063301   \n",
            "X     0.142226  0.206419  0.056890  0.010379  0.160869  0.054200  0.017682   \n",
            "NUM   0.037487  0.020707  0.003570  0.014281  0.119243  0.001428  0.035345   \n",
            "PRT   0.019569  0.401174  0.101370  0.002348  0.045010  0.017613  0.082975   \n",
            "NOUN  0.176827  0.149134  0.013106  0.042454  0.240094  0.004659  0.012584   \n",
            "ADV   0.119472  0.339022  0.071373  0.006982  0.139255  0.012025  0.130721   \n",
            "\n",
            "             X       NUM       PRT      NOUN       ADV  \n",
            "ADP   0.034548  0.063275  0.001266  0.323589  0.014553  \n",
            "VERB  0.215930  0.022836  0.030663  0.110589  0.083886  \n",
            "DET   0.045134  0.022855  0.000287  0.635906  0.012074  \n",
            "CONJ  0.009330  0.040615  0.004391  0.349067  0.057080  \n",
            ".     0.025641  0.078210  0.002789  0.218539  0.052569  \n",
            "PRON  0.088383  0.006834  0.014123  0.212756  0.036902  \n",
            "ADJ   0.020971  0.021748  0.011456  0.696893  0.005243  \n",
            "X     0.075726  0.003075  0.185086  0.061695  0.025754  \n",
            "NUM   0.202428  0.184220  0.026062  0.351660  0.003570  \n",
            "PRT   0.012133  0.056751  0.001174  0.250489  0.009393  \n",
            "NOUN  0.028825  0.009144  0.043935  0.262344  0.016895  \n",
            "ADV   0.022886  0.029868  0.014740  0.032196  0.081458  \n",
            "Time taken in seconds:  28.981415033340454\n",
            "Viterbi Algorithm Accuracy:  87.2852233676976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.util import ngrams\n",
        "\n",
        "text = \"\"\"\n",
        "The phone is fast.\n",
        "The phone is smooth.\n",
        "The phone is powerful.\n",
        "The phone is reliable.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = [w.lower() for w in re.findall(r\"\\b\\w+\\b\", text)]\n",
        "\n",
        "# Bigram Model\n",
        "N = 2\n",
        "train_data, vocab = padded_everygram_pipeline(N, [tokens])\n",
        "\n",
        "lm_bigram = MLE(N)\n",
        "lm_bigram.fit(train_data, vocab)\n",
        "\n",
        "print(\"\\nN=2 (Bigram Model)\")\n",
        "bigram_counts = {}\n",
        "for token in set(tokens):\n",
        "    bigram_counts[token] = dict(lm_bigram.counts[(token,)])\n",
        "print(\"All bigram counts:\", bigram_counts)\n",
        "\n",
        "# Prediction - 3 words\n",
        "context = 'the'\n",
        "predicted_words = [context]\n",
        "for i in range(3):\n",
        "    if lm_bigram.counts[(context,)]:\n",
        "        next_word = max(lm_bigram.counts[(context,)].items(), key=lambda x: x[1])[0]\n",
        "        predicted_words.append(next_word)\n",
        "        context = next_word\n",
        "print(f\"Bigram prediction: {' '.join(predicted_words)}\")\n",
        "\n",
        "# Trigram Model\n",
        "N = 3\n",
        "train_data, vocab = padded_everygram_pipeline(N, [tokens])\n",
        "\n",
        "lm_trigram = MLE(N)\n",
        "lm_trigram.fit(train_data, vocab)\n",
        "\n",
        "print(\"\\nN=3 (Trigram Model)\")\n",
        "trigram_counts = {}\n",
        "bigrams_in_text = list(ngrams(tokens, 2))\n",
        "for bg in set(bigrams_in_text):\n",
        "    trigram_counts[bg] = dict(lm_trigram.counts[bg])\n",
        "print(\"All trigram counts:\", trigram_counts)\n",
        "\n",
        "# Prediction - 3 words\n",
        "context = ['the', 'phone'] # context for trigram prediction is a list of the two preceding words\n",
        "predicted_words = context.copy()\n",
        "for i in range(3):\n",
        "    context_tuple = tuple(context)\n",
        "    if context_tuple in lm_trigram.counts: # Check if the context exists\n",
        "        next_word_candidates_counts = lm_trigram.counts[context_tuple]\n",
        "        if next_word_candidates_counts: # Check if there are any next words for this context\n",
        "            # Find the next word with the maximum count\n",
        "            next_word = max(next_word_candidates_counts.items(), key=lambda x: x[1])[0]\n",
        "            predicted_words.append(next_word)\n",
        "            context = [context[1], next_word] # Update context for next prediction\n",
        "        else:\n",
        "            break # Stop if no next word is found\n",
        "    else:\n",
        "        break # Stop if the context is not found in the trigram counts\n",
        "print(f\"Trigram prediction: {' '.join(predicted_words)}\")"
      ],
      "metadata": {
        "id": "hzt3l0F4T-Gd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "96b0799d-6ddf-4d10-98f1-c9e29926ab99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "N=2 (Bigram Model)\n",
            "All bigram counts: {'the': {'phone': 4}, 'reliable': {'</s>': 1}, 'is': {'fast': 1, 'smooth': 1, 'powerful': 1, 'reliable': 1}, 'fast': {'the': 1}, 'smooth': {'the': 1}, 'powerful': {'the': 1}, 'phone': {'is': 4}}\n",
            "Bigram prediction: the phone is fast\n",
            "\n",
            "N=3 (Trigram Model)\n",
            "All trigram counts: {('is', 'fast'): {'the': 1}, ('the', 'phone'): {'is': 4}, ('is', 'smooth'): {'the': 1}, ('is', 'reliable'): {'</s>': 1}, ('is', 'powerful'): {'the': 1}, ('powerful', 'the'): {'phone': 1}, ('phone', 'is'): {'fast': 1, 'smooth': 1, 'powerful': 1, 'reliable': 1}, ('fast', 'the'): {'phone': 1}, ('smooth', 'the'): {'phone': 1}}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'tuple' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1559487506.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mpredicted_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mcontext_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext_tuple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlm_trigram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Check if the context exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mnext_word_candidates_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm_trigram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext_tuple\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
          ]
        }
      ]
    }
  ]
}